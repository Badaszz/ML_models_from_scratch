{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RdIbbAYritq_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "  # Defining the class for tree nodes\n",
        "  def __init__(self, feature_index = None, threshold = None, left = None, right = None, *, value = None ):\n",
        "    self.feature_index = feature_index  # index of the feature used for split\n",
        "    self.threshold = threshold          # threshold for the split\n",
        "    self.left = left                    # left child\n",
        "    self.right = right                  # right child\n",
        "    self.value = value                  # value (final prediction) if it's a leaf node\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "  # Defining the class for the classifier\n",
        "  def __init__(self, max_depth = 5, min_samples_split = 2):\n",
        "    self.root = None\n",
        "    self.max_depth = max_depth\n",
        "    self.min_samples_split = min_samples_split\n",
        "\n",
        "  # method to calculate Gini impurity\n",
        "  # (this is what is used for optimization by the greedy algorithm of DTs)\n",
        "  def gini(self, y):\n",
        "    classes = np.unique(y)\n",
        "    impurity = 1.0\n",
        "    for clas in classes:\n",
        "      p = np.sum(y==clas) / len(y)\n",
        "      impurity -= p**2\n",
        "    return impurity\n",
        "\n",
        "  # Method to find the best split (greedy algorithm)\n",
        "  def best_split(self, X, y):\n",
        "    best_gain = -1\n",
        "    split_idx, split_thresh = None, None\n",
        "    current_impurity = self.gini(y) # initial impurity is impurity of the whole dataset\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    for f_indx in range(n_features):\n",
        "      # Loop through all the features\n",
        "      thresholds = np.unique(X[:, f_indx])\n",
        "      for thresh in thresholds:\n",
        "        # Loop through possible splits in the features\n",
        "        left_idx = np.where(X[:, f_indx] <= thresh)[0]\n",
        "        right_idx = np.where(X[:, f_indx] > thresh)[0]\n",
        "\n",
        "        if len(left_idx) == 0 or len(right_idx) == 0:\n",
        "            continue\n",
        "\n",
        "        left_impurity = self.gini(y[left_idx])\n",
        "        right_impurity = self.gini(y[right_idx])\n",
        "        n = len(y)\n",
        "        n_left, n_right = len(left_idx), len(right_idx)\n",
        "        # Calculate impurity based on this split\n",
        "        weighted_impurity = (n_left / n) * left_impurity + (n_right / n) * right_impurity\n",
        "        # Calculate Info gain, (we want to minimize whole impurity of nodes by maximizing Info Gain)\n",
        "        info_gain = current_impurity - weighted_impurity\n",
        "\n",
        "        if info_gain > best_gain:\n",
        "          # If this split has the best info gain, then this is the best split\n",
        "          best_gain = info_gain\n",
        "          split_idx = f_indx\n",
        "          split_thresh = thresh\n",
        "\n",
        "    return split_idx, split_thresh, best_gain\n",
        "\n",
        "  def build_tree(self, X, y, depth = 0):\n",
        "    # Define recursive function to build trees\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    # condition for stopping\n",
        "    if depth >= self.max_depth or n_labels == 1 or n_samples <self.min_samples_split:\n",
        "      leaf_value = self.majority_class(y)\n",
        "      return Node(value=leaf_value)\n",
        "\n",
        "    # else we find the best split\n",
        "    feature_index, threshold, gain = self.best_split(X,y)\n",
        "    if gain == 0:\n",
        "      # If the gain is 0, then make a leaf node\n",
        "      leaf_value = self.majority_class(y)\n",
        "      return Node(value = leaf_value)\n",
        "\n",
        "    # else we update indexes of remaining data for more splitting\n",
        "    left_idx = np.where(X[:, feature_index] <= threshold)[0]\n",
        "    right_idx = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "    # we call build_function again to continue until the stopping conditions are met\n",
        "    left_subtree = self.build_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "    right_subtree = self.build_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "    return Node(feature_index, threshold, left_subtree, right_subtree)\n",
        "\n",
        "## function for finding majority class for leaf nodes0\n",
        "  def majority_class(self, y):\n",
        "    counts = np.bincount(y)\n",
        "    return np.argmax(counts)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    # Fit tree, with first build as the root\n",
        "    self.root = self.build_tree(X,y)\n",
        "\n",
        "  def _predict(self, node, x):\n",
        "    if node.value is not None:\n",
        "      return node.value\n",
        "    if x[node.feature_index] <= node.threshold:\n",
        "      return self._predict(node.left, x)\n",
        "    else:\n",
        "      return self._predict(node.right, x)\n",
        "\n",
        "  def predict(self, data):\n",
        "    return np.array([self._predict(self.root, x) for x in data])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a simple binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
        "    n_clusters_per_class=1, random_state=42\n",
        ")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth = 7, min_samples_split = 10)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usm4Ocdzvhe0",
        "outputId": "6e40b91b-fa28-4de7-9ca3-10dc762a8d62"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Noice"
      ],
      "metadata": {
        "id": "Af3oufXLwjHj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "25PgOJVwxJei"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}